name: Epic 3 - Automated Backups to S3
on:
  schedule:
    # Run daily at 2:00 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      backup_type:
        description: 'Type of backup to perform'
        required: true
        default: 'full'
        type: choice
        options:
        - 'full'
        - 'database-only'
        - 'files-only'
      site_name:
        description: 'Site name to backup (default: all sites)'
        required: false
        default: 'all'
        type: string
      retention_days:
        description: 'Number of days to retain backups'
        required: false
        default: '30'
        type: string

env:
  AWS_REGION: us-east-1
  TERRAFORM_DIR: iac/aws/ec2/terraform

jobs:
  backup:
    name: Backup ERPNext to S3
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}
        
    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: 1.5.0
        terraform_wrapper: false
        
    - name: Get infrastructure information
      id: get-info
      working-directory: ${{ env.TERRAFORM_DIR }}
      run: |
        terraform init -backend-config=backend-config.tf
        INSTANCE_ID=$(terraform output -raw instance_id)
        BACKUP_BUCKET=$(terraform output -raw backup_bucket_name)
        echo "instance_id=$INSTANCE_ID" >> $GITHUB_OUTPUT
        echo "backup_bucket=$BACKUP_BUCKET" >> $GITHUB_OUTPUT
        echo "Instance ID: $INSTANCE_ID"
        echo "Backup Bucket: $BACKUP_BUCKET"
        
    - name: Prepare backup script
      run: |
        cat > backup-erpnext.sh << 'EOF'
        #!/bin/bash
        set -e
        
        # Configuration
        BACKUP_TYPE="${{ github.event.inputs.backup_type || 'full' }}"
        SITE_NAME="${{ github.event.inputs.site_name || 'all' }}"
        RETENTION_DAYS="${{ github.event.inputs.retention_days || '30' }}"
        BACKUP_BUCKET="${{ steps.get-info.outputs.backup_bucket }}"
        TIMESTAMP=$(date +%Y%m%d_%H%M%S)
        BACKUP_DIR="/tmp/backup_$TIMESTAMP"
        
        echo "=== ERPNext Backup Started ==="
        echo "Backup Type: $BACKUP_TYPE"
        echo "Site: $SITE_NAME"
        echo "Retention: $RETENTION_DAYS days"
        echo "S3 Bucket: $BACKUP_BUCKET"
        echo "Timestamp: $(date)"
        
        # Create backup directory
        mkdir -p $BACKUP_DIR
        cd $BACKUP_DIR
        
        # Check if ERPNext is running
        cd /opt/erpnext/frappe_docker
        if ! docker compose --project-name erpnext -f ~/gitops/docker-compose.yml ps | grep -q "Up"; then
          echo "❌ ERPNext containers are not running. Cannot perform backup."
          exit 1
        fi
        
        # Function to backup a specific site
        backup_site() {
          local site=$1
          echo "Backing up site: $site"
          
          # Create site-specific backup directory
          SITE_BACKUP_DIR="$BACKUP_DIR/$site"
          mkdir -p "$SITE_BACKUP_DIR"
          
          if [ "$BACKUP_TYPE" = "full" ] || [ "$BACKUP_TYPE" = "database-only" ]; then
            echo "Creating database backup for $site..."
            docker compose --project-name erpnext -f ~/gitops/docker-compose.yml exec -T backend \
              bench --site $site backup --compress
              
            # Copy database backup
            docker compose --project-name erpnext -f ~/gitops/docker-compose.yml exec -T backend \
              bash -c "find sites/$site/private/backups -name '*database*.sql.gz' -type f -exec cp {} /tmp/ \;"
              
            docker cp $(docker compose --project-name erpnext -f ~/gitops/docker-compose.yml ps -q backend):/tmp/ "$SITE_BACKUP_DIR/"
            find "$SITE_BACKUP_DIR" -name "*.sql.gz" -exec mv {} "$SITE_BACKUP_DIR/database_$TIMESTAMP.sql.gz" \;
          fi
          
          if [ "$BACKUP_TYPE" = "full" ] || [ "$BACKUP_TYPE" = "files-only" ]; then
            echo "Creating files backup for $site..."
            docker compose --project-name erpnext -f ~/gitops/docker-compose.yml exec -T backend \
              bench --site $site backup --with-files --compress
              
            # Copy files backup
            docker compose --project-name erpnext -f ~/gitops/docker-compose.yml exec -T backend \
              bash -c "find sites/$site/private/backups -name '*files*.tar' -type f -exec cp {} /tmp/ \;"
              
            docker cp $(docker compose --project-name erpnext -f ~/gitops/docker-compose.yml ps -q backend):/tmp/ "$SITE_BACKUP_DIR/"
            find "$SITE_BACKUP_DIR" -name "*files*.tar" -exec mv {} "$SITE_BACKUP_DIR/files_$TIMESTAMP.tar" \;
          fi
          
          # Create site configuration backup
          echo "Backing up site configuration for $site..."
          docker compose --project-name erpnext -f ~/gitops/docker-compose.yml exec -T backend \
            bash -c "cd sites/$site && tar -czf /tmp/config_$site_$TIMESTAMP.tar.gz site_config.json"
            
          docker cp $(docker compose --project-name erpnext -f ~/gitops/docker-compose.yml ps -q backend):/tmp/config_${site}_$TIMESTAMP.tar.gz "$SITE_BACKUP_DIR/"
        }
        
        # Get list of sites to backup
        if [ "$SITE_NAME" = "all" ]; then
          echo "Getting list of all sites..."
          SITES=$(docker compose --project-name erpnext -f ~/gitops/docker-compose.yml exec -T backend \
            bash -c "ls sites/ | grep -v apps.txt | grep -v assets | grep -v common_site_config.json" | tr -d '\r')
        else
          SITES="$SITE_NAME"
        fi
        
        echo "Sites to backup: $SITES"
        
        # Backup each site
        for site in $SITES; do
          if [ ! -z "$site" ]; then
            backup_site "$site"
          fi
        done
        
        # Create system configuration backup
        echo "Creating system configuration backup..."
        SYSTEM_BACKUP_DIR="$BACKUP_DIR/system"
        mkdir -p "$SYSTEM_BACKUP_DIR"
        
        # Backup docker-compose configuration
        cp ~/gitops/docker-compose.yml "$SYSTEM_BACKUP_DIR/"
        cp ~/gitops/erpnext.env "$SYSTEM_BACKUP_DIR/"
        
        # Backup common site config
        docker compose --project-name erpnext -f ~/gitops/docker-compose.yml exec -T backend \
          bash -c "cp sites/common_site_config.json /tmp/"
        docker cp $(docker compose --project-name erpnext -f ~/gitops/docker-compose.yml ps -q backend):/tmp/common_site_config.json "$SYSTEM_BACKUP_DIR/"
        
        # Create comprehensive backup metadata
        cat > "$BACKUP_DIR/backup_metadata.json" << METADATA_EOF
        {
          "backup_id": "$TIMESTAMP",
          "backup_type": "$BACKUP_TYPE",
          "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
          "sites": [$(echo "$SITES" | sed 's/^/"/;s/$/"/;s/ /", "/g')],
          "retention_days": $RETENTION_DAYS,
          "backup_size_mb": $(du -sm $BACKUP_DIR | cut -f1),
          "git_commit": "${GITHUB_SHA:-$(git rev-parse HEAD 2>/dev/null || echo 'unknown')}",
          "git_branch": "${GITHUB_REF_NAME:-$(git branch --show-current 2>/dev/null || echo 'unknown')}"
        }
        METADATA_EOF
        
        # Create backup archive
        cd /tmp
        echo "Creating backup archive..."
        tar -czf "erpnext_backup_$TIMESTAMP.tar.gz" "backup_$TIMESTAMP/"
        
        # Upload to S3
        echo "Uploading backup to S3..."
        aws s3 cp "erpnext_backup_$TIMESTAMP.tar.gz" "s3://$BACKUP_BUCKET/backups/"
        
        # Upload metadata separately for easier querying
        aws s3 cp "$BACKUP_DIR/backup_metadata.json" "s3://$BACKUP_BUCKET/metadata/backup_metadata_$TIMESTAMP.json"
        
        # Calculate backup size
        BACKUP_SIZE=$(stat -c%s "erpnext_backup_$TIMESTAMP.tar.gz")
        BACKUP_SIZE_MB=$((BACKUP_SIZE / 1024 / 1024))
        
        echo "Backup uploaded successfully!"
        echo "Backup file: erpnext_backup_$TIMESTAMP.tar.gz"
        echo "Backup size: ${BACKUP_SIZE_MB}MB"
        echo "S3 location: s3://$BACKUP_BUCKET/backups/erpnext_backup_$TIMESTAMP.tar.gz"
        
        # Cleanup old backups based on retention policy
        echo "Cleaning up old backups (keeping last $RETENTION_DAYS days)..."
        
        # Delete local files older than retention period
        find /tmp -name "erpnext_backup_*.tar.gz" -type f -mtime +$RETENTION_DAYS -delete 2>/dev/null || true
        find /tmp -name "backup_*" -type d -mtime +$RETENTION_DAYS -exec rm -rf {} \; 2>/dev/null || true
        
        # List and optionally clean S3 backups older than retention period
        echo "Checking S3 for old backups..."
        aws s3 ls "s3://$BACKUP_BUCKET/backups/" | while read -r line; do
          backup_date=$(echo $line | awk '{print $1}')
          backup_file=$(echo $line | awk '{print $4}')
          
          if [ ! -z "$backup_date" ] && [ ! -z "$backup_file" ]; then
            backup_epoch=$(date -d "$backup_date" +%s 2>/dev/null || echo 0)
            cutoff_epoch=$(date -d "-$RETENTION_DAYS days" +%s)
            
            if [ $backup_epoch -lt $cutoff_epoch ] && [ $backup_epoch -gt 0 ]; then
              echo "Deleting old backup: $backup_file (date: $backup_date)"
              aws s3 rm "s3://$BACKUP_BUCKET/backups/$backup_file"
              # Also delete corresponding metadata
              metadata_file=$(echo "$backup_file" | sed 's/erpnext_backup_/backup_metadata_/' | sed 's/.tar.gz/.json/')
              aws s3 rm "s3://$BACKUP_BUCKET/metadata/$metadata_file" 2>/dev/null || true
            fi
          fi
        done
        
        # Cleanup local temporary files
        rm -rf "$BACKUP_DIR" "erpnext_backup_$TIMESTAMP.tar.gz"
        
        # Create backup verification script
        cat > /opt/erpnext/verify-last-backup.sh << 'VERIFY_EOF'
        #!/bin/bash
        BACKUP_BUCKET="${{ steps.get-info.outputs.backup_bucket }}"
        
        echo "=== Last Backup Verification ==="
        echo "Timestamp: $(date)"
        
        # Get the most recent backup from S3
        LATEST_BACKUP=$(aws s3 ls "s3://$BACKUP_BUCKET/backups/" | sort | tail -n 1 | awk '{print $4}')
        
        if [ ! -z "$LATEST_BACKUP" ]; then
          echo "Latest backup: $LATEST_BACKUP"
          
          # Get backup metadata
          METADATA_FILE=$(echo "$LATEST_BACKUP" | sed 's/erpnext_backup_/backup_metadata_/' | sed 's/.tar.gz/.json/')
          
          if aws s3 cp "s3://$BACKUP_BUCKET/metadata/$METADATA_FILE" /tmp/latest_metadata.json 2>/dev/null; then
            echo "Backup metadata:"
            cat /tmp/latest_metadata.json | jq .
            rm /tmp/latest_metadata.json
          fi
          
          # Check backup age
          BACKUP_DATE=$(echo "$LATEST_BACKUP" | grep -o '[0-9]\{8\}_[0-9]\{6\}' | head -1)
          if [ ! -z "$BACKUP_DATE" ]; then
            BACKUP_TIMESTAMP=$(echo "$BACKUP_DATE" | sed 's/_/ /' | sed 's/\([0-9]\{4\}\)\([0-9]\{2\}\)\([0-9]\{2\}\) \([0-9]\{2\}\)\([0-9]\{2\}\)\([0-9]\{2\}\)/\1-\2-\3 \4:\5:\6/')
            BACKUP_EPOCH=$(date -d "$BACKUP_TIMESTAMP" +%s 2>/dev/null || echo 0)
            CURRENT_EPOCH=$(date +%s)
            HOURS_OLD=$(( (CURRENT_EPOCH - BACKUP_EPOCH) / 3600 ))
            
            echo "Backup age: $HOURS_OLD hours"
            
            if [ $HOURS_OLD -lt 26 ]; then
              echo "✅ Backup is recent (less than 26 hours old)"
            else
              echo "⚠️  Backup is older than expected ($HOURS_OLD hours)"
            fi
          fi
        else
          echo "❌ No backups found in S3 bucket"
        fi
        
        # Check local backup script status
        if [ -f "/var/log/erpnext-backup.log" ]; then
          echo ""
          echo "Recent local backup log entries:"
          tail -10 /var/log/erpnext-backup.log
        fi
        VERIFY_EOF
        
        chmod +x /opt/erpnext/verify-last-backup.sh
        
        echo "=== ERPNext Backup Completed ==="
        echo "Backup ID: $TIMESTAMP"
        echo "Backup size: ${BACKUP_SIZE_MB}MB"
        echo "Sites backed up: $SITES"
        echo "S3 location: s3://$BACKUP_BUCKET/backups/erpnext_backup_$TIMESTAMP.tar.gz"
        echo "Verification script created: /opt/erpnext/verify-last-backup.sh"
        
        # Store backup information for GitHub Actions
        echo "BACKUP_ID=$TIMESTAMP" >> /tmp/backup_info.env
        echo "BACKUP_SIZE_MB=${BACKUP_SIZE_MB}" >> /tmp/backup_info.env
        echo "BACKUP_S3_URL=s3://$BACKUP_BUCKET/backups/erpnext_backup_$TIMESTAMP.tar.gz" >> /tmp/backup_info.env
        EOF
        
        chmod +x backup-erpnext.sh
        
    - name: Execute backup via SSM
      run: |
        COMMAND_ID=$(aws ssm send-command \
          --instance-ids ${{ steps.get-info.outputs.instance_id }} \
          --document-name "AWS-RunShellScript" \
          --parameters commands="$(cat backup-erpnext.sh | base64 -w 0 | base64 -d)" \
          --query 'Command.CommandId' \
          --output text)
          
        echo "Command ID: $COMMAND_ID"
        echo "command_id=$COMMAND_ID" >> $GITHUB_ENV
        
        # Wait for command to complete (with longer timeout for backup)
        echo "Waiting for backup to complete..."
        aws ssm wait command-executed \
          --command-id $COMMAND_ID \
          --instance-id ${{ steps.get-info.outputs.instance_id }}
          
    - name: Get backup results
      id: backup-results
      run: |
        echo "=== Backup Results ==="
        aws ssm get-command-invocation \
          --command-id ${{ env.command_id }} \
          --instance-id ${{ steps.get-info.outputs.instance_id }} \
          --query 'StandardOutputContent' \
          --output text
          
        echo "=== Error Output (if any) ==="
        aws ssm get-command-invocation \
          --command-id ${{ env.command_id }} \
          --instance-id ${{ steps.get-info.outputs.instance_id }} \
          --query 'StandardErrorContent' \
          --output text
          
        # Check command status
        STATUS=$(aws ssm get-command-invocation \
          --command-id ${{ env.command_id }} \
          --instance-id ${{ steps.get-info.outputs.instance_id }} \
          --query 'Status' \
          --output text)
          
        echo "Command Status: $STATUS"
        
        if [ "$STATUS" != "Success" ]; then
          echo "Backup failed!"
          exit 1
        fi
        
        # Extract backup information from output
        OUTPUT=$(aws ssm get-command-invocation \
          --command-id ${{ env.command_id }} \
          --instance-id ${{ steps.get-info.outputs.instance_id }} \
          --query 'StandardOutputContent' \
          --output text)
          
        BACKUP_ID=$(echo "$OUTPUT" | grep "Backup ID:" | sed 's/.*Backup ID: //')
        BACKUP_SIZE=$(echo "$OUTPUT" | grep "Backup size:" | sed 's/.*Backup size: //' | sed 's/MB.*//')
        S3_URL=$(echo "$OUTPUT" | grep "S3 location:" | sed 's/.*S3 location: //')
        
        echo "backup_id=$BACKUP_ID" >> $GITHUB_OUTPUT
        echo "backup_size=$BACKUP_SIZE" >> $GITHUB_OUTPUT
        echo "s3_url=$S3_URL" >> $GITHUB_OUTPUT
        
    - name: Verify backup in S3
      run: |
        BACKUP_BUCKET="${{ steps.get-info.outputs.backup_bucket }}"
        BACKUP_ID="${{ steps.backup-results.outputs.backup_id }}"
        
        if [ ! -z "$BACKUP_ID" ]; then
          echo "Verifying backup in S3..."
          
          # Check if backup file exists in S3
          if aws s3 ls "s3://$BACKUP_BUCKET/backups/erpnext_backup_$BACKUP_ID.tar.gz" > /dev/null; then
            echo "✅ Backup file verified in S3"
            
            # Get file size from S3
            S3_SIZE=$(aws s3 ls "s3://$BACKUP_BUCKET/backups/erpnext_backup_$BACKUP_ID.tar.gz" | awk '{print $3}')
            S3_SIZE_MB=$((S3_SIZE / 1024 / 1024))
            echo "S3 backup size: ${S3_SIZE_MB}MB"
            
            # Check metadata file
            if aws s3 ls "s3://$BACKUP_BUCKET/metadata/backup_metadata_$BACKUP_ID.json" > /dev/null; then
              echo "✅ Backup metadata verified in S3"
              
              # Download and display metadata
              aws s3 cp "s3://$BACKUP_BUCKET/metadata/backup_metadata_$BACKUP_ID.json" /tmp/metadata.json
              echo "Backup metadata:"
              cat /tmp/metadata.json | jq .
            else
              echo "⚠️  Backup metadata not found in S3"
            fi
          else
            echo "❌ Backup file not found in S3"
            exit 1
          fi
        else
          echo "❌ Could not determine backup ID"
          exit 1
        fi
        
    - name: Send backup notification
      if: always()
      run: |
        BACKUP_STATUS="${{ job.status }}"
        BACKUP_ID="${{ steps.backup-results.outputs.backup_id }}"
        BACKUP_SIZE="${{ steps.backup-results.outputs.backup_size }}"
        S3_URL="${{ steps.backup-results.outputs.s3_url }}"
        
        if [ "$BACKUP_STATUS" = "success" ]; then
          echo "✅ Backup completed successfully"
        else
          echo "❌ Backup failed"
        fi
        
        # Here you could add notification logic (email, Slack, etc.)
        # For now, we'll just log the result
        echo "Backup notification: Status=$BACKUP_STATUS, ID=$BACKUP_ID, Size=${BACKUP_SIZE}MB"
        
    - name: Create backup summary
      if: always()
      run: |
        BACKUP_STATUS="${{ job.status }}"
        BACKUP_ID="${{ steps.backup-results.outputs.backup_id }}"
        BACKUP_SIZE="${{ steps.backup-results.outputs.backup_size }}"
        S3_URL="${{ steps.backup-results.outputs.s3_url }}"
        
        cat >> $GITHUB_STEP_SUMMARY << EOF
        # 💾 ERPNext Backup Summary
        
        ## Backup Details
        - **Status**: $([ "$BACKUP_STATUS" = "success" ] && echo "✅ Success" || echo "❌ Failed")
        - **Backup ID**: $BACKUP_ID
        - **Backup Type**: ${{ github.event.inputs.backup_type || 'full' }}
        - **Sites**: ${{ github.event.inputs.site_name || 'all' }}
        - **Backup Size**: ${BACKUP_SIZE}MB
        - **Executed At**: $(date -u)
        - **Retention**: ${{ github.event.inputs.retention_days || '30' }} days
        
        ## Storage Information
        - **S3 Bucket**: ${{ steps.get-info.outputs.backup_bucket }}
        - **S3 Location**: $S3_URL
        - **Metadata**: Available in S3 metadata/ folder
        
        ## Backup Contents
        $([ "${{ github.event.inputs.backup_type || 'full' }}" = "full" ] && echo "- ✅ Database backup
        - ✅ Files backup" || echo "- Database or files backup as requested")
        - ✅ Site configuration
        - ✅ System configuration
        - ✅ Backup metadata
        
        ## Next Steps
        1. Backup is automatically retained for ${{ github.event.inputs.retention_days || '30' }} days
        2. Older backups are automatically cleaned up
        3. Use /opt/erpnext/verify-last-backup.sh to verify latest backup
        4. Monitor daily backup schedule (2:00 AM UTC)
        
        ## Restoration
        To restore from this backup, use the backup ID: \`$BACKUP_ID\`
        EOF
